{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Stopword function\n",
    "\n",
    "\n",
    "def stopword(string):\n",
    "    from nltk.corpus import stopwords\n",
    "    res = [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(res)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Download and import tokenizer libraries\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Initialize lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Tokenize Summaries\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    res = [wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(res)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Text pre-processing\n",
    "def get_pre_preprocessing(df1,col):\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    import numpy as np\n",
    "    df = df1.copy()\n",
    "# Convert Summary data to type(str)\n",
    "    df[col] = df[col].apply(lambda x: str(x))\n",
    "# Remove html tags and leading 'b's\n",
    "    df[col] = df[col].apply(lambda row: re.compile(r'<.*?>').sub(r' ',row))\n",
    "# Remove special characters, replace with space\n",
    "    df[col] = df[col].apply(lambda row: re.compile(r'\\W').sub(r' ',row))\n",
    "# Remove single characters\n",
    "    df[col] = df[col].apply(lambda row: re.compile(r'\\s+[a-zA-Z]\\s+').sub(r' ',row))\n",
    "# Remove stil characters from start\n",
    "    df[col] = df[col].apply(lambda row: re.compile(r'\\^[a-zA-Z]\\s+').sub(r' ',row))\n",
    "# Clean up multi-space gaps\n",
    "    df[col] = df[col].apply(lambda row: re.compile(r'\\s+').sub(r' ',row))\n",
    "# Converting to lower case\n",
    "    df[col] = df[col].apply(lambda row: row.lower())\n",
    "# Apply stopwords func to Summaries\n",
    "    df[col] = df[col].apply(lambda text: stopword(text))\n",
    "# Apply lemmatizer func to Summaries\n",
    "    df[col] = df[col].apply(lambda text: lemmatizer(text))\n",
    "# Initialize tokenizer and tokenize summary data\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#apply tokinizer to text data\n",
    "    df[col] = df[col].apply(tokenizer.tokenize)\n",
    "# Create token length count column\n",
    "    df['token_count'] = [len(x) for x in df[col]]\n",
    "# Calculate standard deviation of token counts\n",
    "    print(pd.DataFrame.std(df.token_count, ddof=2))\n",
    "# Checking standard deviation using np\n",
    "    arr = pd.Series.to_numpy(df.token_count)\n",
    "    print(np.std(arr, ddof=2))\n",
    "#add the tokenize column to the original column \n",
    "    df1['token'] = df[col]\n",
    "    return df1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}